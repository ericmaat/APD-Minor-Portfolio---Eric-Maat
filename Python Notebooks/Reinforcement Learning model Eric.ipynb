{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98a97325",
   "metadata": {},
   "source": [
    "# In this Notebook...\n",
    "- Reinforcement Learning\n",
    "    - Environment\n",
    "    - Replay Buffer\n",
    "    - Neural Network\n",
    "    - Agent\n",
    "    - Hyperparameters\n",
    "    - Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf51b1eb",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6431c0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rewards and Punishments\n",
    "\n",
    "#rewards:\n",
    "#containers with same prio close to each other\n",
    "#keep stacks as low as possible\n",
    "\n",
    "#punishments:\n",
    "#amount of moves it takes to get all containers of one prio class out of created lay-out\n",
    "#container with lower prio on top of container with high prio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a71cdacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remaining checklist\n",
    "\n",
    "#environment: game over when shiplist empty\n",
    "#new score function: calculate score by counting moves it takes to remove containers from layout\n",
    "#checkmiddle function: when 4 or more wide, fix that stack can't be placed in middle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e8fe67",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5cc1b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using gpu 0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cfef410",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Is GPU avaible? if not, use cpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85594474",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fedc890",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateEnvironment(size=[3,3,2]):\n",
    "    c =  [[['0'] * size[2]] * size[1]] * size[0]\n",
    "    cSize = size[0]*size[1]*size[2]*3\n",
    "    actionSize = size[0]*size[1]*size[2]\n",
    "    speelveld = np.array(c)\n",
    "    return speelveld, size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c776193e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[['0', '0'],\n",
       "         ['0', '0'],\n",
       "         ['0', '0']],\n",
       " \n",
       "        [['0', '0'],\n",
       "         ['0', '0'],\n",
       "         ['0', '0']],\n",
       " \n",
       "        [['0', '0'],\n",
       "         ['0', '0'],\n",
       "         ['0', '0']]], dtype='<U1'),\n",
       " [3, 3, 2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generateEnvironment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ded0b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0, 0)\n"
     ]
    }
   ],
   "source": [
    "#From X,Y,Z coördinates to index in array\n",
    "def coord_to_ind(X, Y, Z, lot_size):\n",
    "    grid = np.arange(np.prod(lot_size)).reshape(lot_size)\n",
    "    ind = grid[X][Y][Z]\n",
    "    return ind\n",
    "\n",
    "#From index in array to X,Y,Z coördinates\n",
    "def ind_to_coord(ind, lot_size):\n",
    "    grid = np.arange(np.prod(lot_size)).reshape(lot_size)\n",
    "    coord = np.where(grid[:,:,:] == ind)\n",
    "    X = coord[0][0]\n",
    "    Y = coord[1][0]\n",
    "    Z = coord[2][0]\n",
    "    return X, Y, Z\n",
    "\n",
    "#test some code\n",
    "c = ind_to_coord(0, [1,1,1])\n",
    "e = generateEnvironment()\n",
    "a = AddContainer(e[0],e[1],1,c[0],c[1],c[2])\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55c219a",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54f2ab36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class environ:\n",
    "    \n",
    "    def __init__(self, lot_size=generateEnvironment()[1]):\n",
    "        if not isinstance(lot_size, (list, tuple)) or not len(lot_size) == 3:\n",
    "            raise ValueError('UNACCEPTABLE LOT_SIZE >:(')\n",
    "            \n",
    "        self.lot_size = lot_size\n",
    "        self.max_x = lot_size[0]-1\n",
    "        self.max_y = lot_size[1]-1\n",
    "        self.max_z = lot_size[2]-1\n",
    "        self.speelveld = generateEnvironment()[0]\n",
    "        self.speelveld_ind = np.prod(lot_size)\n",
    "         \n",
    "    \n",
    "    def step(self, act):#'act' moet int zijn\n",
    "        \n",
    "        #reward = 0\n",
    "        act_loc = ind_to_coord(act, self.lot_size)\n",
    "        new_speelveld = AddContainer(self.speelveld, self.lot_size, random.randint(1,3), act_loc[0], act_loc[1], act_loc[2])\n",
    "        \n",
    "        #game_over when yard is full, no more containers can be added to yard or ship is empty\n",
    "        gameover = True\n",
    "        for a in range(self.max_x+1):\n",
    "            if new_speelveld[a][0][self.max_z] == '0' or new_speelveld[a][self.max_y][self.max_z] == '0':\n",
    "                gameover = False\n",
    "                break\n",
    "        \n",
    "        \n",
    "        if not gameover:\n",
    "            #place container on avaible space\n",
    "            if not (self.speelveld == new_speelveld).all():\n",
    "                reward = 1\n",
    "                self.speelveld = new_speelveld\n",
    "            #place container on unavaible space\n",
    "            else:\n",
    "                reward = -1\n",
    "                self.speelveld = new_speelveld\n",
    "                \n",
    "        #no container can be added to lot\n",
    "        else:\n",
    "            #containeryard is filled\n",
    "            if not '0' in new_speelveld:\n",
    "                reward = 3\n",
    "                self.reset()\n",
    "            #containeryard still had space\n",
    "            else:\n",
    "                reward = -3\n",
    "                self.reset()\n",
    "            \n",
    "        return new_speelveld, reward, gameover #array, int, bool\n",
    "    \n",
    "    #reset yard to empty\n",
    "    def reset(self):\n",
    "        \n",
    "        self.speelveld = generateEnvironment()[0]\n",
    "        return self.speelveld\n",
    "    #def render(self):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c25deb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Environment Restrictions (from Martti)\n",
    "    #-------------------------------------------------------------\n",
    "    #Checks if current position is not floating.\n",
    "    def CheckBelow(Environment, X, Y, Z):\n",
    "        return Z == 0 or (Environment[X,Y,:Z] != '0').all()\n",
    "\n",
    "    #Checks if current position has container.\n",
    "    def CheckPos(Environment, Size, X, Y, Z):\n",
    "        if not ExceedsLot(Size, X, Y, Z):\n",
    "            return Environment[X, Y, Z] != '0'\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    #Checks if coordinates exceed lot.\n",
    "    def ExceedsLot(Size, X, Y, Z):\n",
    "        return Size[2] <= Z or Size[1] <= Y or Size[0] <= X or Z < 0 or Y < 0 or X < 0\n",
    "\n",
    "    #FIX, when 4 or more wide, fix that stack can't be placed in middle\n",
    "    #Check whether container is in the middle of other containers.\n",
    "    def CheckMiddle(Environment, Size, X, Y, Z):\n",
    "        return CheckPos(Environment, Size, X, Y - 1, 0) and CheckPos(Environment, Size, X, Y + 1, 0)\n",
    "\n",
    "    #Adds container to environment using state.\n",
    "    def AddContainer(Environment, Size, State, X, Y, Z):\n",
    "        tempEnvironment = np.copy(Environment)\n",
    "        if CheckBelow(Environment, X, Y, Z) and not CheckPos(Environment, Size, X, Y, Z) and not ExceedsLot(Size, X, Y, Z) and not CheckMiddle(Environment, Size, X, Y, Z):\n",
    "            tempEnvironment[X,Y,Z] = State\n",
    "        return tempEnvironment\n",
    "    #--------------------------------------------------------------   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be314f17",
   "metadata": {},
   "source": [
    "# Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f017675",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, size_max=10000):\n",
    "        self.storagelst = []\n",
    "        self.size_max = size_max\n",
    "        self.ptr = 0\n",
    "    \n",
    "    #Add data to storage\n",
    "    def add(self, data):#data contains state, state_next, action, reward and done\n",
    "        #when storage reach max, replace first in storage\n",
    "        if len(self.storagelst) == self.size_max:\n",
    "            self.storagelst[int(self.ptr)] = data\n",
    "            self.ptr = (self.ptr + 1) % self.size_max\n",
    "        #add to storage\n",
    "        else:\n",
    "            self.storagelst.append(data)\n",
    "    \n",
    "    #take random sample from storage list\n",
    "    def sample(self, batch_size):\n",
    "        ind = np.random.randint(0, len(self.storagelst), size=batch_size)\n",
    "        st, stn, act, rew, don = [], [], [], [], []\n",
    "        \n",
    "        for i in ind: \n",
    "            S, N, A, R, D = self.storage[i]\n",
    "            st.append(np.array(S, copy=False))\n",
    "            stn.append(np.array(N, copy=False))\n",
    "            act.append(np.array(A, copy=False))\n",
    "            rew.append(np.array(R, copy=False))\n",
    "            don.append(np.array(D, copy=False))\n",
    "        \n",
    "        return np.array(st), np.array(stn), np.array(act).reshape(-1,1), np.array(rew).reshape(-1,1), np.array(don).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dfb801",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0420ec8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DQN\n",
    "class NeurNet(nn.Module):\n",
    "    def __init__(self, inputsize, outputsize, hiddensize):\n",
    "        super(NeurNet, self).__init__()\n",
    "        self.denselayer_1 = nn.Linear(inputsize, hiddensize)\n",
    "        self.denselayer_2 = nn.Linear(hiddensize, hiddensize)\n",
    "        self.output = nn.Linear(hiddensize, outputsize)\n",
    "        \n",
    "        #self.loss = nn.MSEloss()\n",
    "        #self.optimizer = optim.Adam(self.parameters(), lr=lrate)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.denselayer_1(x))\n",
    "        x = F.relu(self.denselayer_2(x))\n",
    "        return self.output(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baaf248",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "176de508",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, inputsize, outputsize, hiddensize, lot_space, lrate):\n",
    "        self.actionspace = [i for i in range(lot_space)]\n",
    "        self.outputsize = outputsize\n",
    "        self.trainnet = NeurNet(inputsize, outputsize, hiddensize).to(device)\n",
    "        self.targnet = NeurNet(inputsize, outputsize, hiddensize).to(device)\n",
    "        self.targnet.load_state_dict(self.trainnet.state_dict())\n",
    "        self.optimizer = optim.Adam(self.trainnet.parameters(), lr=lrate)\n",
    "        \n",
    "    def action(self, state, epsilon):\n",
    "        #if epsilon still high, higher chance of random\n",
    "        if np.random.rand() <= epsilon:\n",
    "            act = random.choice(self.actionspace)\n",
    "        \n",
    "        #put input into neural network and get an action\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                inputstate = torch.FloatTensor(state).to(device)\n",
    "                act = self.trainnet(inputstate).max(0)[1].view(-1)\n",
    "                act = int(action)\n",
    "        return act #index of place in array where container will be put (int)\n",
    "    \n",
    "    \n",
    "    def train(self, replay_buffer, batch_size, disc_gamma):\n",
    "        #take sample from replay buffer storagelist\n",
    "        st_nu, st_next, act, rew, done = replay_buffer.sample(batch_size)\n",
    "        \n",
    "        #batches to tensors\n",
    "        state_batch = torch.FloatTensor(st_nu).to(device)\n",
    "        next_state_batch = torch.FloatTensor(st_next).to(device)\n",
    "        action_batch = torch.LongTensor(act).to(device)\n",
    "        reward_batch = torch.FloatTensor(rew).to(device)\n",
    "        done_batch = torch.FloatTensor(1 - done).to(device)\n",
    "        \n",
    "        #get train and target net Q values\n",
    "        train_q = self.trainnet(state_batch).gather(1, action_batch)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            targnet_q = reward_batch + done_batch * disc_gamma * \\\n",
    "                     torch.max( self.targnet(next_state_batch).detach(), dim=1)[0].view(batch_size, -1)\n",
    "        \n",
    "        #loss function\n",
    "        loss_func = nn.MSELoss()\n",
    "        loss = loss_func(train_q, targnet_q)\n",
    "        #optimize parameters\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.detach().cpu().numpy()\n",
    "    \n",
    "    #update target network\n",
    "    def update_netw(self, num_iter, update_every):\n",
    "        #only update sometimes\n",
    "        if num_iter % update_every == 0:\n",
    "            self.targnet.load_state_dict(self.trainnet.state_dict())\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839cd935",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b05a050",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "hidden_size = 64 #hiddenlayer size of neurnet\n",
    "episodes = 1000 #amount of games\n",
    "start_train_after = 100 #after this many steps start training\n",
    "learning_rate = 0.0005\n",
    "update_target_by = 1000 #update target network after this many steps\n",
    "batch_size = 64 # batch size used for train network\n",
    "discount_gamma = 0.9\n",
    "\n",
    "#epsilon\n",
    "epsilon_start = 1.0\n",
    "epsilon_min = 0.001\n",
    "epsilon_decay_by = episodes * 0.15\n",
    "epsilon_decrease = (epsilon_start - epsilon_min)/(epsilon_decay_by)\n",
    "\n",
    "#make environment\n",
    "lot_size = generateEnvironment()[1]\n",
    "env = environ(lot_size)\n",
    "inputsize = lot_size[0] * lot_size[1] * lot_size[2]\n",
    "outputsize = lot_size[0] * lot_size[1] * lot_size[2]\n",
    "lotspace = lot_size[0] * lot_size[1] * lot_size[2]\n",
    "\n",
    "#make replay buffer\n",
    "replay_size = 10000\n",
    "replay_buffer = ReplayBuffer(max_size=replay_size)\n",
    "\n",
    "#make agent\n",
    "agent = Agent(inputsize,outputsize,hidden_size,lotspace,learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5d199c",
   "metadata": {},
   "source": [
    "# Loop Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "49549775",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9561/4183677267.py:26: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(x), np.array(y), np.array(u).reshape(-1,1), np.array(r).reshape(-1,1), np.array(d).reshape(-1,1)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [125], line 48\u001b[0m\n\u001b[1;32m     46\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timesteps \u001b[38;5;241m>\u001b[39m start_train_after:\n\u001b[0;32m---> 48\u001b[0m     stats_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscount_gamma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     agent\u001b[38;5;241m.\u001b[39mupdate_netw(timesteps, update_target_by)\n",
      "Cell \u001b[0;32mIn [119], line 23\u001b[0m, in \u001b[0;36mAgent.train\u001b[0;34m(self, replay_buffer, batch_size, discount)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, replay_buffer, batch_size, discount):\n\u001b[1;32m     21\u001b[0m     st_nu, st_next, act, rew, done \u001b[38;5;241m=\u001b[39m replay_buffer\u001b[38;5;241m.\u001b[39msample(batch_size)\n\u001b[0;32m---> 23\u001b[0m     state_batch \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFloatTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mst_nu\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     24\u001b[0m     next_state_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(st_next)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     25\u001b[0m     action_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mLongTensor(act)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool."
     ]
    }
   ],
   "source": [
    "list_stat_rewards = []\n",
    "stats_by = 10\n",
    "total_reward = 0\n",
    "timesteps = 0\n",
    "episode_len = 0\n",
    "epsilon = epsilon_start\n",
    "\n",
    "for ep in range(episodes):\n",
    "    state = env.reset()\n",
    "    state_loss = 0\n",
    "    \n",
    "    #train each episode\n",
    "    while True:\n",
    "        timesteps += 1\n",
    "        #select action\n",
    "        action = agent.action(state, epsilon)\n",
    "        #apply action to environment\n",
    "        next_state, reward, done = env.step(action)\n",
    "        total_reward += reward\n",
    "        episode_len += 1\n",
    "        \n",
    "        #when agent is done\n",
    "        if done:\n",
    "            next_state = np.zeros(state.shape)\n",
    "            reward = -1\n",
    "            #save experience to replay buffer\n",
    "            replay_buffer.add((state, next_state, action, reward, done))\n",
    "            \n",
    "            list_stat_rewards.append((ep, total_reward, episode_len))\n",
    "            total_reward = 0\n",
    "            episode_len = 0\n",
    "            \n",
    "            epsilon -= epsilon_decrease\n",
    "            if epsilon < epsilon_min:\n",
    "                epsilon = epsilon_min\n",
    "                \n",
    "            if timesteps > start_train_after and ep % stats_by == 0:\n",
    "                print('Episode: {}'.format(ep),\n",
    "                    'Timestep: {}'.format(timesteps),\n",
    "                    'Total reward: {:.1f}'.format(np.mean(list_stat_rewards[-stats_every:],axis=0)[1]),\n",
    "                    'Episode length: {:.1f}'.format(np.mean(list_stat_rewards[-stats_every:],axis=0)[2]),\n",
    "                    'Epsilon: {:.2f}'.format(epsilon),\n",
    "                    'Loss: {:.3f}'.format(stats_loss))\n",
    "            break\n",
    "        \n",
    "        #in both cases, save experience to replay buffer\n",
    "        else:\n",
    "            replay_buffer.add((state, next_state, action, reward, done))\n",
    "            \n",
    "        state = next_state\n",
    "        \n",
    "        #if enough steps have been completed to start training\n",
    "        if timesteps > start_train_after:\n",
    "            \n",
    "            #train agent\n",
    "            stats_loss += agent.train(replay_buffer, batch_size, discount_gamma)\n",
    "            #update network\n",
    "            agent.update_netw(timesteps, update_target_by)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92d0424",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
